{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Part 2: Preprocessing & First Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Images from files\n",
    "\n",
    "This is the same as in ./data-exploration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pizza images: 983\n",
      "Number of non-pizza images: 983\n",
      "Total number of images: 1966\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# define the directories where the imgaes are\n",
    "pizza_directory=\"./pizza/\"\n",
    "not_pizza_directory=\"./not_pizza/\"\n",
    "\n",
    "# get the images from the directories\n",
    "not_pizza_files=[[0, not_pizza_directory, i] for i in os.listdir(not_pizza_directory)]\n",
    "pizza_files=[[1, pizza_directory, i] for i in os.listdir(pizza_directory)]\n",
    "\n",
    "print(\"Number of pizza images:\",len(pizza_files))\n",
    "print(\"Number of non-pizza images:\",len(not_pizza_files))\n",
    "\n",
    "# combine the image sets\n",
    "all_files = not_pizza_files + pizza_files\n",
    "\n",
    "print(\"Total number of images:\", len(all_files))\n",
    "\n",
    "# each element in all_files is of the format [is_pizza: boolean, directory: string, filename: string]\n",
    "# in other words, for each `img` in `all_files`:\n",
    "#    - img[0] = 0 if not pizza, 1 if pizza\n",
    "#    - img[1] = directory where the image is\n",
    "#    - img[2] = file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "Resize all images to 512x512 and put them into a new directory.\n",
    "\n",
    "Create the following arrays:\n",
    "\n",
    "- **X** - array of the pixel rgb values of the resized images. Each element is in the shape **[512][512][3]**\n",
    "- **y** - array of whether the corresponding image is of a pizza or not. Each element is a **Boolean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "standardSize = (512,512)\n",
    "resizedImages = list()\n",
    "isPizzaArray = list()\n",
    "\n",
    "# structure is:\n",
    "# resizedImages[i] = 2-D numpy array of pixels, [512][512][3]\n",
    "\n",
    "\n",
    "PATH = os.path.abspath(os.getcwd())\n",
    "if (not os.path.exists(f'{PATH}/resizedImages')):\n",
    "    os.mkdir('resizedImages')\n",
    "\n",
    "for i, image in enumerate(all_files):\n",
    "    imgData = Image.open(image[1] + image[2])\n",
    "    imgData = imgData.resize(standardSize)\n",
    "    imgData.save(f'{PATH}/resizedImages/{i}.jpg')\n",
    "    # storing images\n",
    "    resizedImages.append(np.array(imgData))\n",
    "    isPizzaArray.append(image[0])\n",
    "\n",
    "# ! Takes about 30 seconds...\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert resizedImages and y to numpy arrary for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x,y: 1966 1966\n",
      "(1966, 512, 512, 3) (1966,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(resizedImages)\n",
    "y = np.array(isPizzaArray)\n",
    "print(\"x,y:\",len(X),len(y))\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Model\n",
    "\n",
    "A very simple model to test if we processed our data correctly and to see how the simple model performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into testing and training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1572, 512, 512, 3)\n",
      "(394, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[122, 124,  94],\n        [123, 125,  95],\n        [125, 128,  99],\n        ...,\n        [165, 144,  97],\n        [166, 144,  97],\n        [167, 145,  98]],\n\n       [[127, 131,  98],\n        [128, 132,  99],\n        [130, 133, 103],\n        ...,\n        [165, 143,  98],\n        [166, 144,  97],\n        [166, 144,  97]],\n\n       [[136, 139, 105],\n        [137, 140, 106],\n        [138, 141, 108],\n        ...,\n        [164, 141,  97],\n        [164, 142,  95],\n        [164, 142,  95]],\n\n       ...,\n\n       [[108,  97,  59],\n        [105,  95,  59],\n        [100,  93,  58],\n        ...,\n        [161,  60,  11],\n        [160,  58,   9],\n        [159,  57,   8]],\n\n       [[110,  98,  60],\n        [107,  96,  60],\n        [103,  95,  58],\n        ...,\n        [160,  56,   8],\n        [165,  59,  11],\n        [171,  64,  17]],\n\n       [[110,  98,  60],\n        [108,  95,  60],\n        [103,  95,  58],\n        ...,\n        [160,  54,   6],\n        [169,  63,  15],\n        [181,  73,  26]]], dtype=uint8)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model and fit training data. First svm model, then neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (None)/charset_normalizer (3.0.1) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 16/105 [===>..........................] - ETA: 1:40 - loss: 209.1518"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [6], line 32\u001B[0m\n\u001B[0;32m     29\u001B[0m numEpoches \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     30\u001B[0m printOutput \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m---> 32\u001B[0m modelData \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnumEpoches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "model = Sequential() # Initialising the ANN\n",
    "# build your convulusional layers here:\n",
    "\n",
    "imgShape = (512, 512, 3)\n",
    "\n",
    "model.add(Conv2D(7, 3, input_shape=imgShape, activation = 'tanh'))\n",
    "model.add(Conv2D(3, 3, activation = 'relu'))\n",
    "# model.add(Conv2D(2, 3, activation = 'relu'))\n",
    "\n",
    "\n",
    "# make sure you flatten after you convulusional layers here:\n",
    "model.add(Flatten())\n",
    "\n",
    "# Feel free to add hidden layers here: \n",
    "\n",
    "# model.add(Dense(units = 10, activation = 'relu'))\n",
    "\n",
    "# # Hint reduce to a 1d vector as you have in the previous exercises.\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# compile your model here:\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')\n",
    "\n",
    "numEpoches = 2\n",
    "printOutput = 5\n",
    "\n",
    "modelData = model.fit(X_train, y_train, batch_size = 15, epochs = numEpoches, verbose = 1)\n",
    "\n",
    "#! Takes about 5 minutes for 2 epoches..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report for Testing and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "yhat = model.predict(X_train)\n",
    "yhat = [0 if y<=0.5 else 1 for y in yhat]\n",
    "\n",
    "print('Training Data Classification Report')\n",
    "print(classification_report(y_train, yhat))\n",
    "\n",
    "yhat_test = model.predict(X_test)\n",
    "yhat_test = [0 if y<=0.5 else 1 for y in yhat_test]\n",
    "\n",
    "print('Test Data Classification Report')\n",
    "print(classification_report(y_test, yhat_test))\n",
    "\n",
    "# ! Takes about 45 seconds..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where does our model lie on the fitting curve?\n",
    "\n",
    "We notice that our model overfits our training data, while our model performs slightly better than a coin toss for the testing data.\n",
    "\n",
    "So our model lies in the fitting curve as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='fitting_curve.png') \n",
    "\n",
    "#! warning, not dark mode friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Second Model, using an SVM.\n",
    "To determine best svm kernel we iterated through a random subset of our data and computed the score.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of kernel linear 0.5\n",
      "Mean accuracy of kernel rbf 0.6\n",
      "Mean accuracy of kernel poly 0.5\n",
      "Mean accuracy of kernel sigmoid 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# thing=X_trai[0].reshape(1,X_train[0].size)\n",
    "# thing.shape\n",
    "X_trai=X_train[0:10]\n",
    "y_trai=y_train[0:10]\n",
    "X_tes=X_test[30:50]\n",
    "y_tes=y_test[30:50]\n",
    "X_trai=np.array(X_trai.reshape((X_trai.shape[0], 512*512*3)),dtype=np.uint8)\n",
    "X_tes=np.array(X_tes.reshape((X_tes.shape[0], 512*512*3)),dtype=np.uint8)\n",
    "\n",
    "for i in [\"linear\",\"rbf\",\"poly\",\"sigmoid\"]:\n",
    "    SVM_cl=SVC(kernel=i)\n",
    "    SVM_cl.fit(X_trai,y_trai)\n",
    "    X_tes=X_test[30:50]\n",
    "    y_tes=y_test[30:50]\n",
    "    print(\"Mean accuracy of kernel {} {}\".format(i,SVM_cl.score(X_tes,y_tes)))\n",
    "    #Best appears to be sigmoid"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_cl.predict(X_tes.reshape((X_tes.shape[0], 512*512*3)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implement SVM calc, with Naive Bayes\n",
    "We could not append the svm score of each image to the obseravtion data of the images being fed to the neural net. Because the neural net uses convolution, we could not add more dimensions, or replace one of the tensors with the svm score without causing erronious bias. So instead, we inserted the SVM classification and neural net classification into a Categorical Naive Bayes classifier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 326ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([1.], dtype=float32)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing=model.predict(X_trai)\n",
    "np.round(thing[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "np.random.seed(1)\n",
    "\n",
    "\"Preparing data\"\n",
    "X_trai=X_train[0:20]\n",
    "y_trai=y_train[0:20]\n",
    "X_tes=X_test[0:10]\n",
    "y_tes=y_test[0:10]\n",
    "\"Create and Run NN\"\n",
    "model = Sequential() # Initialising the ANN\n",
    "imgShape = (512, 512, 3)\n",
    "model.add(Conv2D(7, 3, activation = 'tanh',input_shape=imgShape))\n",
    "model.add(Conv2D(3, 2, activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation=\"sigmoid\")) #Here is the difference. Each pixel will push the image above or below the SVM constraints.\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',metrics=[\"accuracy\"])\n",
    "numEpoches = 3\n",
    "model.fit(X_trai, y_trai, batch_size = 5, epochs = 3, verbose = 1)\n",
    "NN_SVM_pred_train=[[np.round(i)] for i in model.predict(X_trai)]\n",
    "NN_SVM_pred_test=[[np.round(i)] for i in model.predict(X_tes)]\n",
    "\n",
    "\"Create and Run SVM\"\n",
    "X_trai_SVM=np.array(X_trai.reshape((X_trai.shape[0], 512*512*3)),dtype=np.uint8)\n",
    "X_tes_SVM=np.array(X_tes.reshape((X_tes.shape[0], 512*512*3)),dtype=np.uint8)\n",
    "SVM_cl=SVC(kernel=\"sigmoid\")\n",
    "SVM_cl.fit(X_trai_SVM,y_trai)\n",
    "for i,ii in enumerate(SVM_cl.predict(X_tes_SVM)):\n",
    "    NN_SVM_pred_test[i].append(ii)\n",
    "for i,ii in enumerate(SVM_cl.predict(X_trai_SVM)):\n",
    "    NN_SVM_pred_train[i].append(ii)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         6\n",
      "           1       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.38      0.38      0.38        10\n",
      "weighted avg       0.40      0.40      0.40        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:845: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order)\n",
      "c:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:845: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"Create and Run Naive\"\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import classification_report\n",
    "CNB_obj=CategoricalNB()                     #initialized object\n",
    "CNB_obj.fit(X=NN_SVM_pred_train,y=y_trai)\n",
    "print(classification_report(y_tes,CNB_obj.predict(NN_SVM_pred_test))) #classification report\n",
    "\n",
    "#! Takes about 5 minutes for 2 epoches..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "yhat = model.predict(X_train)\n",
    "yhat = [0 if y<=0.5 else 1 for y in yhat]\n",
    "print('Training Data Classification Report')\n",
    "print(classification_report(y_train, yhat))\n",
    "yhat_test = model.predict(X_test)\n",
    "yhat_test = [0 if y<=0.5 else 1 for y in yhat_test]\n",
    "\n",
    "print('Test Data Classification Report')\n",
    "print(classification_report(y_test, yhat_test))\n",
    "# ! Takes about 45 seconds..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
