{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.decomposition\n",
    "\n",
    "pd.DataFrame #creates a Dataframe object of the selected input. Creates a 2D table, with rows as dimension 0, columns dimension 1, and each column is selectable as a string.\n",
    "# df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],       #the data/observations\n",
    "# ...      index=['cobra', 'viper', 'sidewinder'],  #the row lables==index\n",
    "# ...      columns=['max_speed', 'shield'])         #the column labels=attributes\n",
    "pd.isnull() #returns a truth table of where values are missing in the table.\n",
    "pd.read_csv() #reads csv data from a csv file format.\n",
    "pd.get_dummies(data: Any, dummy_na: bool= False, columns:Any = None) #Does the onehot encoding itself. Convert categorical variables of X_train into dummy/indicator variables.\n",
    "\n",
    "pd.DataFrame.dropna() #Return DataFrame with labels on given axis omitted where (all or any) data are missing.\n",
    "pd.DataFrame.drop_duplicates() #Return Dataframe with duplicate rows removed.\n",
    "pd.DataFrame.drop(labels: Any = None, axis: str | int = 0, index: Any = None, columns: Any = None,inplace: bool = False,) #drops certain columns of the panda data from, returns a pandas object without the columns. columns= also works.\n",
    "pd.DataFrame.info() #returns attributes, how many null values each table has, and datatype in each cell.\n",
    "pd.DataFrame.describe() #displays table.\n",
    "pd.DataFrame.loc() #access groups of rows or columns by label. You enter \"label\" or [\"labels\",\"labels\"], and it will search rows and columns.\n",
    "#df.loc[(df['class'] != 2)] #return dataframe where the values of the class attribute are not 2.\n",
    "pd.DataFrame.replace(to_replace=\"values in the df to replace\",value=\"what to replace the df values with\")# You can search a column and replace values by column, but this method replaces everything in the dataframe.\n",
    "#df = df.replace({\"class\": {0: -1}}) #in class, replace 0 with -1.\n",
    "pd.DataFrame.plot.scatter(x='feature1', y='feature2') #builtin method of creating a scatter plot form data.\n",
    "pd.DataFrame.corr() #returns a correlation table of each of the features.\n",
    "pd.DataFrame.head(n=)#returns the top n rows.\n",
    "pd.DataFrame.hist(figsize=(16,16), grid=False) #returns the histogram of each feature.\n",
    "pd.DataFrame.boxplot([\"columns\"]) #returns the boxplot of features\n",
    "pd.Dataframe[\"column\"].value_counts() #returns the counts of categorical, or numerical data in a column\n",
    "pd.DataFrame.reset_index(inplace=True) #resents the index of a dataframe,\n",
    "pd.DataFrame.set_index(keys=,inplace=True) #Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). Keys can be either a single column key, a single array of the same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays.\n",
    "pd.DataFrame.copy(deep=True) #Creates an actual copy, not just an assignment.\n",
    "pd.DataFrame.nunique(self,axis: str | int = 0, dropna: bool = True)) #Count distinct elems in axis. Return Series with number of distinct elements. Can ignr NaN, when dropna is True.\n",
    "pd.DataFrame.keys() #returns the keys, or column names, of the object. == to .columns."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Seaborn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style() #sets the style of the plot about to write. Color style \"whitegrid\"\n",
    "sns.scatterplot(x=,y=,hue=,data=)# X and Y specify coordinates of specific variables. hue differentiates points based on categorical or numerical data. data is the object, such as a pd.dataframe or numpy array, that x and y come from.\n",
    "sns.heatmap(data=,annot=) #heatmap of values,ranging from max to min of the data. Sould normalize data beforehand.\n",
    "sns.pairplot(data=,vars=) #plots pairwise relationship of data. Plots each feature as x and y, diagonal shows a histogram of each feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'units'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[0;32m      2\u001B[0m model_name\u001B[38;5;241m=\u001B[39mkeras\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mSequential() \u001B[38;5;66;03m#creates an instance of the keras neural net. Need to add layers.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\gerri\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\dtensor\\utils.py:96\u001B[0m, in \u001B[0;36mallow_initializer_layout.<locals>._wrap_function\u001B[1;34m(layer_instance, *args, **kwargs)\u001B[0m\n\u001B[0;32m     93\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m layout:\n\u001B[0;32m     94\u001B[0m             layout_args[variable_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_layout\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m layout\n\u001B[1;32m---> 96\u001B[0m init_method(layer_instance, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     98\u001B[0m \u001B[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layout_param_name, layout \u001B[38;5;129;01min\u001B[39;00m layout_args\u001B[38;5;241m.\u001B[39mitems():\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 1 required positional argument: 'units'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.models.Sequential() #creates an instance of the keras neural net. Need to add layers.\n",
    "keras.layers.Dense() #layer format.\n",
    "keras.models.Sequential.add()\n",
    "keras.models.Sequential.compile(optimizer='rmsprop',loss=None,metrics=None,weighted_metrics=None,steps_per_execution=None,jit_compile=None,)\n",
    "keras.models.Sequential.predict_on_batch()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras metrics\n",
    "AUC: Approximates the AUC (Area under the curve) of the ROC or PR curves.\n",
    "\n",
    "Accuracy: Calculates how often predictions equal labels.\n",
    "\n",
    "BinaryAccuracy: Calculates how often predictions match binary labels.\n",
    "\n",
    "BinaryCrossentropy: Computes the crossentropy metric between the labels and predictions.\n",
    "\n",
    "BinaryIoU: Computes the Intersection-Over-Union metric for class 0 and/or 1.\n",
    "\n",
    "CategoricalAccuracy: Calculates how often predictions match one-hot labels.\n",
    "\n",
    "CategoricalCrossentropy: Computes the crossentropy metric between the labels and predictions.\n",
    "\n",
    "CategoricalHinge: Computes the categorical hinge metric between y_true and y_pred.\n",
    "\n",
    "CosineSimilarity: Computes the cosine similarity between the labels and predictions.\n",
    "\n",
    "FalseNegatives: Calculates the number of false negatives.\n",
    "\n",
    "FalsePositives: Calculates the number of false positives.\n",
    "\n",
    "Hinge: Computes the hinge metric between y_true and y_pred.\n",
    "\n",
    "IoU: Computes the Intersection-Over-Union metric for specific target classes.\n",
    "\n",
    "KLDivergence: Computes Kullback-Leibler divergence metric between y_true and\n",
    "\n",
    "LogCoshError: Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "Mean: Computes the (weighted) mean of the given values.\n",
    "\n",
    "MeanAbsoluteError: Computes the mean absolute error between the labels and predictions.\n",
    "\n",
    "MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true and"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras loss\n",
    "class BinaryCrossentropy: Computes the cross-entropy loss between true labels and predicted labels. The CategoricalCrossentropy also computes the cross-entropy loss between the true classes and predicted classes. The labels are given in an one_hot format.\n",
    "\n",
    "class BinaryFocalCrossentropy: Computes the focal cross-entropy loss between true labels and predictions.\n",
    "\n",
    "class CategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "class CategoricalHinge: Computes the categorical hinge loss between y_true & y_pred.\n",
    "\n",
    "class CosineSimilarity: Computes the cosine similarity between labels and predictions.\n",
    "\n",
    "class Hinge: Computes the hinge loss between y_true & y_pred.\n",
    "\n",
    "class Huber: Computes the Huber loss between y_true & y_pred.\n",
    "\n",
    "class KLDivergence: Computes Kullback-Leibler divergence loss between y_true & y_pred.\n",
    "\n",
    "class LogCosh: Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "class Loss: Loss base class.\n",
    "\n",
    "class MeanAbsoluteError: Computes the mean of absolute difference between labels and predictions.\n",
    "\n",
    "class MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true & y_pred.\n",
    "\n",
    "class MeanSquaredError: Computes the mean of squares of errors between labels and predictions.\n",
    "\n",
    "class MeanSquaredLogarithmicError: Computes the mean squared logarithmic error between y_true & y_pred.\n",
    "\n",
    "class Poisson: Computes the Poisson loss between y_true & y_pred.\n",
    "\n",
    "class Reduction: Types of loss reduction.\n",
    "\n",
    "class SparseCategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
    "y_true = [0, 1,2]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1],[0.1, 0.8, 0.1]]\n",
    "\n",
    "class SquaredHinge: Computes the squared hinge loss between y_true & y_pred."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\"class version\"\n",
    "class LinearSVM:\n",
    "    def __init__(self, soft=False, C=1):\n",
    "        self.relax = soft\n",
    "        self.C = C\n",
    "        self.M = None # number of features/dimensions\n",
    "        self.N = None # number of training data points\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        self.w = None #our weight. 13 weights\n",
    "        self.b = None\n",
    "        self.e = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        (self.N, self.M) = X.shape\n",
    "        self.X = np.asarray(X)\n",
    "        self.y = np.asarray(y).reshape(-1, 1)\n",
    "\n",
    "        w = cp.Variable((self.M, 1))\n",
    "        b = cp.Variable()\n",
    "        e = cp.Variable((self.N, 1))\n",
    "\n",
    "        if self.relax: # Soft-margin SVM\n",
    "            objective = cp.Minimize(cp.sum(cp.square(w)) * 0.5 + cp.sum(e) * self.C)\n",
    "            constraints = [cp.multiply(self.y, (self.X @ w + b)) >= 1 - e, e >=0] #constraints of how many can be within the margin. So...\n",
    "        else: # Hard-margin SVM\n",
    "            objective = cp.Minimize(cp.sum(cp.square(w)) * 0.5) #this is minimum(1/2(w^2)\n",
    "            constraints = [cp.multiply(self.y, (self.X @ w + b)) >= 1] #this is the constraint. Objective is reached, so long as y*support vector is above 1.\n",
    "\n",
    "        prob = cp.Problem(objective, constraints)\n",
    "\n",
    "        prob.solve()\n",
    "        self.w = w.value\n",
    "        self.b = b.value\n",
    "        self.e = e.value\n",
    "        print(f'Status: {prob.status}')\n",
    "        print(f\"Objective Value: {prob.value}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign((np.asarray(X) @ self.w + self.b))\n",
    "\n",
    "    def getSupportVec(self):\n",
    "        if self.relax: # Soft-margin SVM\n",
    "            return (self.y * (self.X @ self.w + self.b) - 1 + self.e <= 1e-10)\n",
    "        else: # Hard-margin SVM\n",
    "            return (self.y * (self.X @ self.w + self.b) - 1 <= 1e-10)\n",
    "    def printSupportVec(self):\n",
    "        \"lower vector {}*({} X {} + {})-1+{}\".format(self.y,self.X,self.w,self.b,self.e)\n",
    "    def getParams(self):\n",
    "        return self.w"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"sklearn version\"\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "scaler = preprocessing.StandardScaler() #initiates a scaler class. Replaces all values in a population with a zscore.\n",
    "scaler.fit_transform() #fits and transforms the inputed data into a zscore.\n",
    "clf = SVC(kernel='linear',)              #initiaties a support vector machine class for linear regression. Can fit and predict on its own.\n",
    "clf_li = LinearSVC(dual=False,loss=\"hinge\")          #creates a linear classification object. Can fit and predict on its own.\n",
    "#Generally,linearSVC() handles scalability better.\n",
    "clf.support_vectors_() #returns the support vectors, or the points which make the support vectors.\n",
    "clf.decision_function()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# sklearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (96600107.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn [2], line 2\u001B[1;36m\u001B[0m\n\u001B[1;33m    sklearn.decomposition.PCA(n_components=number of data attributes)#initialize PCA class\u001B[0m\n\u001B[1;37m                                                  ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.decomposition.PCA(n_components=number of data attributes)#initialize PCA class\n",
    "pca=sklearn.decomposition.PCA.fit()#returns the instance.\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC%s' % _ for _ in range(len(df_normalized.columns))], index=df.columns)\n",
    "sklearn.decomposition.PCA.components_ ##gives you the PCA weights.\n",
    "sklearn.decomposition.PCA.explained_variance_#how much of the variance in the data points is explained by the PCA\n",
    "sklearn.decomposition.PCA.explained_variance_ratio_#tells how much of the variance in the datapoints is explained by the PCA out of the others.\n",
    "sklearn.metrics.classification_report(ytrue=,y_pred=) #returns a report of all the accuracy, ect, of the data\n",
    "sklearn.metrics.confusion_matrix(y_true=,y_pred=) #creates a confusion matrix, to see how many times a category or prediction was wrong, and how many times it was labeled as a certain other thing.\n",
    "encoder = preprocessing.OrdinalEncoder() # Encodes each category as integers. From 0 to n_classes - 1\n",
    "y_encoder = preprocessing.LabelEncoder() # Same functionality but designed for the dependent variable.\n",
    "deg = 5\n",
    "\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=deg, include_bias=False) #Setup the model\n",
    "demo = sklearn.preprocessing.PolynomialFeatures.fit_transform(np.arange(5).reshape(-1, 1))#enriching the data, by transforming each feature, by raising each feature to the degree.\n",
    "\n",
    "#Regularization is one way to prevent overfitting.\n",
    "# The idea is to penalize the coefficients so that you are discouraged to learn a complex model. <br >\n",
    "# To achieve so, we would add a regularization term to the loss function.\n",
    "#first fit the data to transformation.\n",
    "X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
    "# Polynomial Regression with l1-regularization\n",
    "polyRegL1 = sklearn.linear_model.Lasso(alpha=0.2).fit(X_poly, y.reshape(-1, 1))\n",
    "# Polynomial Regression with l2-regularization\n",
    "polyRegL2 = sklearn.linear_model.Ridge(alpha=0.2).fit(X_poly, y.reshape(-1, 1))\n",
    "TREE=sklearn.tree.DecisionTreeClassifier(criterion=\"gini or information gain\", max_depth=3, random_state=0) # max depth is how many levels. Creats a classification tree, and you can insert your own split calculation function.\n",
    "clf_en = sklearn.tree.DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) #This uses criterion entropy, which is just the information gain.\n",
    "sklearn.metrics.accuracy_score() #In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.vstack\n",
    "np.dot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# imblearn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import imblearn\n",
    "imblearn.over_sampling.RandomOverSampler(data)#Randomly over samples the data, by taking multiple rows and duplicating them, while leaving the original."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normal distribution: Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "\n",
    "When copying, or creating dropped dataframes, then you need to use the deep copy method. Shallow copy is when you have an array X, then select parts of an array to be Y. Mess with X, you mess with Y. But if you deep copy the indeces, then you can mess with X, without messing with Y."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 5.,  8., 11., 14., 17., 20.])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.linspace(5,20,6)\n",
    "display(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([11., 14.])"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11. 14.]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([20., 22., 24., 26., 28., 30.])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y=x[2:4:1]\n",
    "display(y)\n",
    "x=np.linspace(20,30,6)\n",
    "print(y)\n",
    "display(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "something=[1 if i>thing else 0 for i in yhat]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Category Coders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
